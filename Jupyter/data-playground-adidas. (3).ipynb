{
  "metadata": {
    "name": "data-playground-adidas",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Case Study Adidas\n\n## This Notebook  xxx\n\n\n### Data Cleaning\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark_conf_list\u003d spark.sparkContext.getConf().getAll()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfor i in spark_conf_list:\n    print(i)\n    "
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nconf \u003d spark.sparkContext._conf.setAll([(\u0027spark.executor.memory\u0027, \u00274g\u0027), \n                                        (\u0027spark.app.name\u0027, \u0027case-study-adidas\u0027),\n                                        (\u0027spark.executor.cores\u0027, \u00274\u0027),\n                                        (\u0027spark.cores.max\u0027, \u00274\u0027), \n                                        (\u0027spark.driver.memory\u0027,\u00274g\u0027)])\nspark \u003d SparkSession.builder.config(conf\u003dconf).getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\ngit clone https://github.com/Prashast18/data-playground-adidas.git\n\n#Zeppelin Notebooks needs to be exported to .ipynb file before pushing to Git\ncp notebook/data-playground-adidas.zpln data-playground-adidas/\ncd data-playground-adidas/\ngit add -A\ngit commit -m \"Init Commit from Zeppelin\"\ngit push"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\nspark.submit.deployMode client"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf\u003d  spark.read.json(\"s3://my-aws-staging-bucket/ol_cdump/ol_cdump.json\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(df.limit(10))"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsummary_df \u003d df.describe()\nz.show(summary_df.limit(5))\n\n# Most of it doesn\u0027t make sense, except the fact that we have:\n\n# Present in all the data\n\"\"\" Revision\n Latest_revision\n Key\n \n \n \nNot Present\n Website\n fuller_name\n full_title\n ia_loaded_id\n bio\n birth_date\n death _rate\n personal_name - can be a trivia\"\"\"\n "
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(df.describe(\"latest_revision\", \"revision\", \"number_of_pages\" ,  \"weight\"))\n\n#Min \u0026 Max Revision - 1, 50\n# Book with Zero page - 0 \u0026 48418(both look unrealistic)\n`\n#"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf2.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#Find count for empty, None, Null, Nan with string literals\n\nfrom pyspark.sql.functions import col,isnan,when,count\n\ndf.filter_cols \u003d [\"bio\", \"birth_date\", \"by_statement\",\"key\", \"latest_revision\", \"title\", \"publish_country\"]\ndf2 \u003d df.select([count(when(col(c).contains(\u0027None\u0027) | \\\n                            col(c).contains(\u0027NULL\u0027) | \\\n                            (col(c) \u003d\u003d \u0027\u0027 ) | \\\n                            col(c).isNull() | \\\n                            isnan(c), c \n                           )).alias(c)\n                    for c in df.filter_cols])"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf2.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nnull_cols \u003d [\"title\", \"number_of_pages\",\"publish_date\", \"publish_country\" ]\ndf.na.drop(subset\u003dnull_cols) \\\n   .count()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\"\"\"Website\n fuller_name\n full_title\n ia_loaded_id\n bio\n birth_date\n death _rate\n personal_name \"\"\""
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#Valid Publish date \u0026 country\n#Not without title\n#valid Birth Date\n#umber of pages - 0\n#Publish year \u003e 1950\n\n\ncleaned_df \u003d df."
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.select(\u0027authors.key\u0027,\u0027authors.author.key\u0027,\u0027publish_date\u0027,\u0027first_publish_date\u0027, \u0027work_title\u0027,\u0027work_titles\u0027, \u0027subject_time\u0027,\u0027subject_times\u0027).show(1000)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n## Merging Similar columns including authors which will get used further.\nbase_df \u003d df.\\\nwithColumn(\u0027oclc_number\u0027,coalesce(\u0027oclc_number\u0027,\u0027oclc_numbers\u0027)).\\\nwithColumn(\u0027subject_time\u0027,coalesce(\u0027subject_time\u0027,\u0027subject_times\u0027)).\\\nwithColumn(\u0027work_title\u0027,coalesce(\u0027work_title\u0027,\u0027work_titles\u0027)).\\\nwithColumn(\u0027publish_date\u0027,coalesce(\u0027publish_date\u0027,\u0027first_publish_date\u0027)).\\\nwithColumn(\u0027authors\u0027, coalesce(\u0027authors.key\u0027,\u0027authors.author.key\u0027)).\\\ndrop(\u0027oclc_numbers\u0027,\u0027subject_times\u0027,\u0027work_titles\u0027,\u0027first_publish_date\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.na.fill(\"Unknown\", [\"bio\", \"birth_date\"]).select(\"bio\", \"birth_date\").show(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr, count, to_timestamp, year, regexp_extract, coalesce, from_json, col, when, explode, length, size, lit, current_date, trim, dense_rank, regexp_replace, split, countDistinct, to_date, concat_ws\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.window import Window"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nw \u003d Window.orderBy(col(\u0027number_of_pages\u0027).desc())\nfirst_df \u003d df.select(\u0027title\u0027,\u0027number_of_pages\u0027,dense_rank().over(w).alias(\u0027rank_num_pages\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfirst_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.filter((df.number_of_pages.isNotNull())| (df.number_of_pages !\u003d 0)).select(\u0027title\u0027,\u0027number_of_pages\u0027).orderBy(df.number_of_pages.desc()).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.filter((df.number_of_pages.isNotNull()) \u0026 (df.number_of_pages !\u003d 0)).select(\u0027title\u0027,\u0027number_of_pages\u0027).orderBy(df.number_of_pages.asc()).limit(3).show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfirst_df.filter(col(\u0027page_rank\u0027) \u003d\u003d 1).select(\u0027title\u0027,\u0027number_of_pages\u0027).show(9,False)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfirst_df.filter(first_df.number_of_pages.isNotNull()).orderBy(first_df.number_of_pages.asc()).select(\u0027title\u0027,\u0027number_of_pages\u0027).show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\n\nselect max(number_of_pages) from book_info\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nspark.sql(\"\"\"select title, author,\n                    description,\n                    publish_date,\n                    RANK() OVER (PARTITION BY title\n                    ORDER BY number_of_pages) AS rank from book_info\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf.registerTempTable(\"books_info\")"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npython -m pip install pandas"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\nimport sys\nsys.path.append(\"/usr/local/lib/python3.7/site-packages\")\nprint(sys.path)"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.dropDuplicates().count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}