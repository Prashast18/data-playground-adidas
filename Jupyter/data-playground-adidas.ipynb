{
  "metadata": {
    "name": "data-playground-adidas",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Case Study Adidas\n\n## This Notebook demonstrates the capabilities of Zeppelin Notebooks taking into account relevant examples for data cleansing, pre-processing and loading the data to an external warehouse.\n\n### Pre-requisites\n#####         EMR Cluster with Zeppelin\n#####         IAM Roles with appropriate permissions - EC2, S3\n#####         Data from Open Library Loaded to S3 bucket\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Initialising the Spark Session"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark_conf_list\u003d spark.sparkContext.getConf().getAll()\nfor i in spark_conf_list:\n    print(i)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nconf \u003d spark.sparkContext._conf.setAll([(\u0027spark.executor.memory\u0027, \u00274g\u0027), \n                                        (\u0027spark.app.name\u0027, \u0027case-study-adidas\u0027),\n                                        (\u0027spark.executor.cores\u0027, \u00274\u0027),\n                                        (\u0027spark.cores.max\u0027, \u00274\u0027), \n                                        (\u0027spark.driver.memory\u0027,\u00274g\u0027)])\nspark \u003d SparkSession.builder.config(conf\u003dconf).enableHiveSupport().getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark.conf\n#spark.submit.deployMode client"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\ncd /mnt/var/lib/zeppelin/git-repo\ngit clone https://github.com/Prashast18/data-playground-adidas.git .\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\ncd /mnt/var/lib/zeppelin/git-repo\ngit clone https://github.com/Prashast18/data-playground-adidas.git .\n\ngit checkout -b dev origin/master\nchmod -R 777 /mnt/var/lib/zeppelin/git-repo/Zeppelin/"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\ncp /mnt/var/lib/zeppelin/notebook/* /mnt/var/lib/zeppelin/git-repo/Zeppelin/"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\n\n#Default Location for  Zeppelin Notebooks: \n#       /mnt/var/lib/zeppelin/notebook/\n\n#Zeppelin Notebooks needs to be exported to .ipynb file before pushing to Git\n\ncp /mnt/var/lib/zeppelin/notebook/ /mnt/var/lib/zeppelin/git-repo/Zeppelin/\ngit add -A\ngit commit -m \"Commit from Zeppelin\"\ngit push origin"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf\u003d  spark.read.json(\"s3://my-aws-staging-bucket/ol_cdump/ol_cdump.json\")"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(df.limit(10))"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\ndf.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nsummary_df \u003d df.describe()\nz.show(summary_df.limit(5))\n\n\"\"\" Content Discovery:\n\n1- The following fields are present in all the data\n                Revision\n                Latest_revision\n                Key\n \n2 -Almost all empty, can be treated as less relevant columns.\n                website\n                fuller_name\n                full_title\n                ia_loaded_id\n                bio\n                birth_date\n                death _rate\n                personal_name (can be a trivia)\n                \n\"\"\"\n "
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nz.show(df.describe(\"latest_revision\", \"revision\", \"number_of_pages\" ,  \"weight\"))\n\n\"\"\"Statistical Analysis Observations\n            1- Min \u0026 Max Revision - 1, 50\n            2- lates_revision \u0026 revision hold the same data\n            3- Book with Zero page - 0 \u0026 48418 (both look unrealistic)\n\"\"\""
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#Find count for empty, None, Null, Nan with string literals\n\nfrom pyspark.sql.functions import col,isnan,when,count\n\ndf.filter_cols \u003d [\"bio\", \"birth_date\", \"by_statement\",\"key\", \"latest_revision\", \"title\", \"publish_country\"]\n\ncount_null_df \u003d df.select([count(when(col(c).contains(\u0027None\u0027) | \\\n                            col(c).contains(\u0027NULL\u0027) | \\\n                            (col(c) \u003d\u003d \u0027\u0027 ) | \\\n                            col(c).isNull() | \\\n                            isnan(c), c \n                           )).alias(c)\n                    for c in df.filter_cols])"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncount_null_df.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Publish Country can be ommitted, but number of authors who co-authored becomes irrelevant\nnull_cols \u003d [\"title\", \"number_of_pages\",\"publish_date\" ]\n\ncleaned_df\u003d df.na.drop(subset\u003dnull_cols)\n\n\n# Based on the business logic, we can fill the columns with some static value\nmassaged_df \u003d df.na.fill(\"Unknown\", [\"bio\", \"birth_date\"]).select(\"bio\", \"birth_date\").show(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncleaned_df.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\"\"\" Following columns do not values in the given data, thay can be used for detecting the outliers, but we are dropping these columns for simplicity\n                website\n                fuller_name\n                full_title\n                ia_loaded_id\n                bio\n                birth_date\n                death _rate\n                personal_name\n                full_title\n                fuller_name\n                ia_box_id\n                ia_loaded_id\n                isbn_invalid\n                isbn_odd_length\n \"\"\"\n \n \nunimp_cols \u003d [ \u0027full_title\u0027,\u0027fuller_name\u0027,\u0027ia_box_id\u0027, \u0027ia_loaded_id\u0027, \u0027isbn_invalid\u0027, \u0027isbn_odd_length\u0027, \u0027website\u0027, \u0027bio\u0027,\u0027birth_date\u0027, \u0027death_rate\u0027 , \u0027personal_name\u0027]\ncleaned_df \u003d cleaned_df.drop(*unimp_cols)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncleaned_df.select(\u0027authors.key\u0027,\u0027authors.author.key\u0027,\u0027publish_date\u0027,\u0027first_publish_date\u0027, \u0027work_title\u0027,\u0027work_titles\u0027, \u0027subject_time\u0027,\u0027subject_times\u0027).show(100)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n\"\"\"                 Valid Publish date \u0026 country\n                    Not without title\n                    Valid Birth Date\n                    Number of pages - 0\n                    Publish year \u003e 1950 \n                    Valid Publish Year  - Publish Year has Junk Values - needs to be cleaned\n\"\"\"\n\ncleaned_df \u003d cleaned_df.withColumn(\u0027publish_year\u0027,regexp_extract(\u0027publish_date\u0027, r\u0027(\\d{4})\u0027, 1))\nbase_df \u003d cleaned_df.filter((cleaned_df[\"publish_year\"] \u003e\u003d \u00271950\u0027) \u0026 \n                            (cleaned_df[\"publish_year\"] \u003c\u003d \u00272022\u0027) \u0026 \n                            (cleaned_df[\"title\"] !\u003d \u0027\u0027) \u0026 \n                            (df[\"number_of_pages\"] \u003e 0))"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbase_df.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr, count, to_timestamp, year, regexp_extract, coalesce, from_json, col, when, explode, length, size, lit, current_date, trim, dense_rank, regexp_replace, split, countDistinct, to_date, concat_ws\nfrom pyspark.sql.types import StructType, StructField, StringType\nfrom pyspark.sql.window import Window"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n## Merge Columns with similar values\n\nbase_df \u003d base_df.\\\n            withColumn(\u0027oclc_number\u0027,coalesce(\u0027oclc_number\u0027,\u0027oclc_numbers\u0027)).\\\n            withColumn(\u0027subject_time\u0027,coalesce(\u0027subject_time\u0027,\u0027subject_times\u0027)).\\\n            withColumn(\u0027work_title\u0027,coalesce(\u0027work_title\u0027,\u0027work_titles\u0027)).\\\n            withColumn(\u0027publish_date\u0027,coalesce(\u0027publish_date\u0027,\u0027first_publish_date\u0027)).\\\n            withColumn(\u0027authors\u0027, coalesce(\u0027authors.key\u0027,\u0027authors.author.key\u0027)).\\\n            drop(\u0027oclc_numbers\u0027,\u0027subject_times\u0027,\u0027work_titles\u0027,\u0027first_publish_date\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbase_df.select(\u0027title\u0027,\u0027number_of_pages\u0027).orderBy(df.number_of_pages.desc()).limit(1).show(truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nw \u003d Window.orderBy(col(\u0027number_of_pages\u0027).desc())\nmin_page_df \u003d base_df.select(\u0027title\u0027,\u0027number_of_pages\u0027,dense_rank().over(w).alias(\u0027rank_num_pages\u0027))\nmin_page_df.filter(col(\u0027rank_num_pages\u0027) \u003d\u003d 1).select(\u0027title\u0027,\u0027number_of_pages\u0027).show(9,False)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# %pyspark\n# base_df.registerTempTable(\"books_info\")\n# spark.sql(\"\"\"select title, author,\n#                     description,\n#                     publish_date,\n#                     RANK() OVER (PARTITION BY title\n#                     ORDER BY number_of_pages) AS rank from book_info\"\"\")"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npython3 -m pip install pandas"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%python\n\n\nimport sys\nsys.path.append(\"/usr/local/lib/python3.7/site-packages\")\nsys.path.append(\"/var/lib/zeppelin/.local/bin\")\nprint(sys.path)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npython3 -m pip list"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#Sample Genre\n        #[Outlines, syllabi, etc]\n\n\nbase_df.select(\u0027title\u0027,explode(\u0027genres\u0027).alias(\u0027genres\u0027)).select(\u0027title\u0027,trim(regexp_replace(\u0027genres\u0027, \u0027[^A-Za-z0-9, ]\u0027,\u0027\u0027)).alias(\u0027genres\u0027)).groupBy(\u0027genres\u0027).\\\nagg(count(\u0027title\u0027).alias(\u0027book_count\u0027)).\\\norderBy(col(\u0027book_count\u0027).desc()).\\\nshow(200,truncate\u003d False)\n\n# Observation\n        #Multi-Lingual Text with same information"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sh\npython3 -m pip install langdetect\npython3 -m pip install googletrans"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "// %pyspark\n\n// from langdetect import detect\n// print(detect(\"Ficcin juvenil\"))\n\n// from googletrans import Translator, constants\n// from pprint import pprint\n\n// translator \u003d Translator()\n// translation \u003d translator.translate(\"Ficcin juvenil\")\n// print(f\"{translation.origin} ({translation.src}) --\u003e {translation.text} ({translation.dest})\")"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#Clean Data DF and replace [.]\n\nexploded_genre_df \u003dbase_df.select(\u0027title\u0027,explode(\u0027genres\u0027).alias(\u0027genres\u0027)).select(\u0027title\u0027,trim(regexp_replace(\u0027genres\u0027, \u0027[^A-Za-z0-9, ]\u0027,\u0027\u0027)).alias(\u0027genres\u0027))"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nexploded_genre_df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntop_genre\u003d exploded_genre_df.withColumn(\"genres\",  when(exploded_genre_df.genres \u003d\u003d \"Literatura juvenil\",\"Juvenile literature\").\n                                                            when(exploded_genre_df.genres \u003d\u003d \"Ficcin\",\"Fiction\").\n                                                            when(exploded_genre_df.genres \u003d\u003d \"Ficcin juvenil\",\"Juvenile fiction\").\n                                                            when(exploded_genre_df.genres \u003d\u003d \"Humorismo\", \"Humor\").\n                                                            otherwise(exploded_genre_df.genres)).groupBy(\u0027genres\u0027).\\\nagg(count(\u0027title\u0027).alias(\u0027book_count\u0027)).\\\norderBy(col(\u0027book_count\u0027).desc()).limit(5)\n\nz.show(top_genre)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nbase_df.filter(size(\u0027authors\u0027) \u003e 1).select(\u0027authors\u0027).show(1000, truncate\u003dFalse)"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nco_auth_df \u003d base_df.filter(size(\u0027authors\u0027) \u003e 1).select(\u0027title\u0027,explode(\u0027authors\u0027).alias(\u0027authors\u0027))\n\n\nco_auth_df.groupBy(\u0027authors\u0027).\\\nagg(count(\u0027title\u0027).alias(\u0027book_count\u0027)).\\\norderBy(col(\u0027book_count\u0027).desc()).\\\nshow(5,False)\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\npubl_books_df \u003d base_df.select(\u0027publish_date\u0027,explode(\u0027authors\u0027).alias(\u0027authors\u0027)).distinct()\n\npubl_books_df.groupBy(\u0027publish_date\u0027).\\\nagg(count(\u0027authors\u0027).alias(\u0027no_of_authors\u0027)).\\\norderBy(col(\u0027publish_date\u0027).desc()).\\\nshow(9999,False)"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# Filter Date between 1950 and 1970\nbooks_pub_r_df \u003d base_df.filter((col(\u0027publish_year\u0027) \u003e\u003d lit(\u00271950\u0027)) \u0026 (col(\u0027publish_year\u0027) \u003c\u003d lit(\u00271970\u0027))).select(explode(\u0027authors\u0027).alias(\u0027authors\u0027),\u0027title\u0027,\u0027publish_year\u0027,\u0027publish_date\u0027)\n\n\n# Extract Publish month\nbooks_pub_r_df \u003dbooks_pub_r_df.withColumn(\u0027publish_month\u0027, regexp_extract(\u0027publish_date\u0027,r\u0027([A-Z][a-z]+)\u0027, 1))\n\n\n# Group by Month and Year, and count distinct title in every group\nbooks_pub_r_df.groupBy(concat_ws(\u0027 \u0027,\u0027publish_month\u0027,\u0027publish_year\u0027).alias(\u0027month_year\u0027)).\\\nagg(countDistinct(\u0027authors\u0027).alias(\u0027authors\u0027),countDistinct(\u0027title\u0027).alias(\u0027books\u0027)).\\\norderBy(concat_ws(\u0027 \u0027,\u0027publish_month\u0027,\u0027publish_year\u0027)).show(99,False)"
    }
  ]
}